{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook used to test concepts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from csv_to_db import ImportCSV\r\n",
    "import pandas as pd\r\n",
    "importcsv = ImportCSV(\"Baes_db\")\r\n",
    "training_data = pd.DataFrame(list(importcsv.db.training_data.find()))\r\n",
    "\r\n",
    "training_data.index.name = \"Steps\"\r\n",
    "#training_data.loc[training_data[\"Result\"] == -1,training_data.columns] #.dropna()\r\n",
    "training_data.to_csv(r\"C:\\Users\\user1\\Desktop\\Bae Rock_paper_scissors\\training_data.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pd.read_csv(r\"C:\\Users\\user1\\Desktop\\Bae Rock_paper_scissors\\data\\increment_results.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "num =1\r\n",
    "num2 = 2\r\n",
    "num = 3 \r\n",
    "pd.DataFrame([[1,2,3]],columns=[\"Num1\",\"Num2\",\"Num3\"])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "d = pd.DataFrame([1,2,3,4],columns=[\"actiobns\"])\r\n",
    "#np.argmax()\r\n",
    "np.max(d[\"actions\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "cpu_rand = [random.randrange(1,4) for i in range(1,4)]\r\n",
    "cpu_rand2 = [random.randrange(1,4) for i in range(1,4)]\r\n",
    "#cpu_action = np.argmax(cpu_rand)\r\n",
    "cpu_ran_state = pd.DataFrame(cpu_rand,columns=[\"action\"],index=[i for i in range(1,4)]) \r\n",
    "cpu_ran_state[\"cpu\"] = cpu_rand2#pd.DataFrame(cpu_rand,columns=[\"action\"]).join(pd.DataFrame) #],index=[i for i in range(1,4)])#.index\r\n",
    "#if cpu_ran_state[\"action\"] == 3:\r\n",
    " #   print(cpu_ran_state.index)\r\n",
    "#display(np.argmax(cpu_ran_state)) #.loc[max(cpu_ran_state[\"action\"]) == 3,[cpu_ran_state.index]]\r\n",
    "display(cpu_ran_state)\r\n",
    "display(cpu_ran_state.idxmax())\r\n",
    "\r\n",
    "#cpu_ran_state.loc[cpu_ran_state[\"action\"] == ]\r\n",
    "#cpu_action\r\n",
    "#cpu_rand"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_data[\"Player_nums\"] = training_data[\"Player\"].replace([\"rock\",\"paper\",\"scissors\"],[1,2,3])\r\n",
    "training_data[\"CPU_nums\"] = training_data[\"CPU\"].replace([\"rock\",\"paper\",\"scissors\"],[1,2,3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# a = Discrete actions: rock=1,paper=2,scissors=3\r\n",
    "# s = State is Player turn: Turn 1, Turn 2, Turn 3\r\n",
    "# r = -1,0,1\r\n",
    "# new_training_data_q_table = training_data(s,a) + A * (r + Y * max(training_data(si+1,a)) - training_data(s,a)  )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 1.Initialise - Player makes turn and CPU adds random number\r\n",
    "# 2. Calculate the Q new learning value "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "class Rock_Paper_Scissors_q:\r\n",
    "    def __init__(self) -> None:\r\n",
    "        self.init_state = 0 \r\n",
    "        self.learning_rate = 0.1\r\n",
    "        self.discount_factor = 0.95\r\n",
    "        self.episodes = 50000\r\n",
    "        self.show_every = 1000\r\n",
    "        self.start_epsilon_decaying = 0.5\r\n",
    "        self.end_epsilon_decaying = self.episodes//10\r\n",
    "        self.epsilon_change = self.epsilon/(self.end_epsilon_decaying  - self.start_epsilon_decaying)\r\n",
    "    def player_step(self,state,api=False,player_move = None):\r\n",
    "        boole = True\r\n",
    "        if api == True:\r\n",
    "            if player_move == \"rock\".lower():\r\n",
    "                player_action = 1\r\n",
    "            elif player_move == \"paper\".lower():\r\n",
    "                player_action = 2\r\n",
    "            elif player_move == \"scissors\".lower():\r\n",
    "                player_action = 3\r\n",
    "            state += 1\r\n",
    "            return state,player_action \r\n",
    "        else:\r\n",
    "            while boole == True:\r\n",
    "                player_move = input(f\"Rock Paper or Scissors:\")\r\n",
    "                if player_move == \"rock\".lower():\r\n",
    "                    player_action = 1\r\n",
    "                    boole = False\r\n",
    "                elif player_move == \"paper\".lower():\r\n",
    "                    player_action = 2\r\n",
    "                    boole = False\r\n",
    "                elif player_move == \"scissors\".lower():\r\n",
    "                    player_action = 3\r\n",
    "                    boole = False\r\n",
    "                elif player_move == \"exit\":\r\n",
    "                    exit()\r\n",
    "                else:\r\n",
    "                    boole = True\r\n",
    "                state += 1\r\n",
    "                return state,player_action \r\n",
    "    \r\n",
    "    def cpu_reward(self,state,player_action,cpu_action):\r\n",
    "        if cpu_action == player_action:\r\n",
    "            reward = 0\r\n",
    "        elif (cpu_action == 1 and player_action == 3) or (cpu_action == 2 and player_action == 1) or (cpu_action == 3 and player_action == 2):\r\n",
    "            reward = 1\r\n",
    "        elif (cpu_action == 2 and player_action == 3) or (cpu_action == 3 and player_action == 1) or (cpu_action == 1 and player_action == 2):\r\n",
    "            reward = -1\r\n",
    "        state += 1\r\n",
    "        return state,reward\r\n",
    "        \r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "    def run_game(self,player_move,q_table,should_update,ind):\r\n",
    "        done = False\r\n",
    "        new_state_init = 0\r\n",
    "\r\n",
    "        # First initialising step\r\n",
    "        # Players Turn initialization\r\n",
    "        #init_state,player_action_init = self.player_step(self.init_state,api=True,player_move=player_move[0])\r\n",
    "        #if np.random.random() > self.epsilon:\r\n",
    "        #    cpu_action = np.argmax(q_table[player_action_init])\r\n",
    "        #else:\r\n",
    "            # Explore - t\r\n",
    "        #    cpu_action = np.random.randint(1,3)\r\n",
    "        # CPU's turn initialization\r\n",
    "        #new_state_init, reward_init = self.cpu_reward(init_state,player_action_init,cpu_action)\r\n",
    "        \r\n",
    "        # Rest of steps\r\n",
    "        while not done:\r\n",
    "            # Players turn \r\n",
    "            new_state_play,player_action = self.player_step(new_state_init,api=True,player_move=player_move[ind])\r\n",
    "            if np.random.random() > self.epsilon:\r\n",
    "                cpu_action = np.argmax(q_table[player_action])\r\n",
    "            else:\r\n",
    "                # Explore - t\r\n",
    "                cpu_action = np.random.randint(1,3)\r\n",
    "            # CPU's turn\r\n",
    "            new_state, reward = self.cpu_reward(new_state_play,player_action,cpu_action)\r\n",
    "            if should_update:\r\n",
    "                max_future_q = np.max(q_table[new_state])\r\n",
    "                current_q = q_table[new_state + (cpu_action,)]\r\n",
    "                new_q =  (1- self.learning_rate) * current_q + self.learning_rate * \\\r\n",
    "                    (reward + self.discount_factor * max_future_q) \r\n",
    "                q_table[new_state_play + (cpu_action,)] = new_q\r\n",
    "            new_state_play = new_state\r\n",
    "            \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Rock_Paper_Scissors_q()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fake_q_table = training_data.drop([\"_id\",\"Player\",\"CPU\"],axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fake_q_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Testing\r\n",
    "# Input: paper,rock\r\n",
    "#Output Next CPU action softmax:  (1,2,3)\r\n",
    "# Players action | CPU actions learns from that "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "model = keras.Sequential()\r\n",
    "model.add()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('roadmapcataloger': conda)"
  },
  "interpreter": {
   "hash": "b96af2ab7256e6e9544468a6b1324d31ee3bbb318d809e21621f6955ed75bb37"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}